{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d303b5",
   "metadata": {},
   "source": [
    "## Project Goal/Question: How might we use self-reported global emissions data to predict Scope 3 (full supply chain) emissions values for similar companies missing emissions data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae646f",
   "metadata": {},
   "source": [
    "Note: This practicum project was a partnership with a private company, so I have omitted outputs for confidentiality reasons.\n",
    "We started with exploratory data analysis (EDA) to get a better sense of the data and how best to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "df = pd.read_csv('C6_5.csv')\n",
    "df.columns = ['Account', 'Organization', 'Country',\t'Public',\t'Response_received_date', 'Primary_activity', 'Primary_sector',\t'Primary_industry', 'Primary_questionnaire_sector', 'Row', 'RowName', 'C6_5_Evaluation_status', 'C6_5_Metric_tonnes_CO2', 'C6.5_Emissions_calculation_methodology', 'C6.5_Percentage_missions_calculated',\t'C6.5_Exclusions']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Metric tonnes CO2 column to numeric\n",
    "df['C6_5_Metric_tonnes_CO2'] = pd.to_numeric(df['C6_5_Metric_tonnes_CO2'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd689ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing values for Evaluation status with \"Not Indicated\"\n",
    "df['C6_5_Evaluation_status'].fillna(\"Not Indicated\", inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c36d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by accounts to see how many accounts are in each Eval status\n",
    "df_sector = df.groupby(['Primary_sector']).count().reset_index()\n",
    "df_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe grouping by sector and eval status\n",
    "df_status = df.groupby(['Primary_sector','C6_5_Evaluation_status'])['Account'].count().reset_index()\n",
    "df_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b00b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status['SectorTotal'] = df_status['Account'].groupby(df_status['Primary_sector']).transform('sum')\n",
    "df_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status['Percent'] = df_status['Account']/df_status['SectorTotal']*100\n",
    "df_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37510f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing values for Percent colum\n",
    "df_status.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing percentages by sector\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(6, 20))\n",
    "reporting = df_status.sort_values(\"Percent\", ascending=False)\n",
    "viz = sns.barplot(x=\"Percent\", y=\"Primary_sector\", data=reporting, hue = \"C6_5_Evaluation_status\", palette=\"mako\", edgecolor=\".1\", dodge=False)\n",
    "sns.move_legend(viz, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusting order of Evaluation Status categories\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(6, 20))\n",
    "viz2 = sns.barplot(x=\"Percent\", y=\"Primary_sector\", data=reporting, hue_order = [\"Question not applicable\", \"Not Indicated\", \"Not evaluated\", \"Not relevant, explanation provided\", \"Relevant, not yet calculated\", \"Not relevant, calculated\", \"Relevant, calculated\"], hue = \"C6_5_Evaluation_status\", palette=\"mako\", edgecolor=\".1\", dodge=False)\n",
    "sns.move_legend(viz2, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking back to actual total numbers of orgs in each sector/status\n",
    "df.groupby('Primary_sector').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking number of orgs providing calculations\n",
    "calculated = [\"Not relevant, calculated\", \"Relevant, calculated\"]\n",
    "df_co2 = df[df['C6_5_Evaluation_status'].isin(calculated)]\n",
    "df_co2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5405d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status2 = df.groupby(['C6_5_Evaluation_status'])['Account'].count().reset_index()\n",
    "df_status2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4615db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at average CO2 for orgs reporting\n",
    "df_mean = df_co2.groupby(['C6_5_Evaluation_status'])['C6_5_Metric_tonnes_CO2'].mean().reset_index()\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daccfb15",
   "metadata": {},
   "source": [
    "###Summary of findings so far\n",
    "\n",
    "Logging and rubber tapping is the only sector with more than 50% of the companies reporting/calcuating CO2 emissions, although there are also a relatively small number of organizations in that category (34). Tobacco is close and then we have a pretty big dropoff to < 40% of organizations in each category reporting calculations of CO2 emissions. Based on these relatively low percentages I wonder how helpful Primary sector will be as a predictor, so perhaps some of the RowName/Greenhouse Gas Protocol categories will be more informative in terms of predicting emissions values. Government agencies are also a huge outlier with no evaluation status even reported, though there are also only 17 government agencies in the dataset.\n",
    "\n",
    "We decided to focus in on the companies that had indicated Scope 3 emissions were relevant and calculated, merging in additional details about their verification processes next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2138197",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_calc = df[df['C6_5_Evaluation_status']=='Relevant, calculated']\n",
    "relevant_calc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing values\n",
    "relevant_calc.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting number of unique accounts\n",
    "df['Account'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ec7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_1 = pd.read_csv('C10.1c.csv')\n",
    "df_10_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_1.columns = [\n",
    "    'Account', 'Organization', 'Country',\t'Public',\t\n",
    "    'Response_received_date', 'Primary_activity', 'Primary_sector',\t\n",
    "    'Primary_industry', 'Primary_questionnaire_sector', 'C10_1c_Row', 'C10_1c_RowName', \n",
    "    'C10_1c_Scope_3_category', 'C10_1c_Verification_cycle', \n",
    "    'C10_1c_Verification_status', 'C10_1c_Verification_type', 'C10_1c_Statement',\n",
    "    'C10_1c_Reference', 'C10_1c_standard', 'C10_1c_Proportion_of_emissions_verified']\n",
    "df_10_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at response options for verification status\n",
    "df_10_1['C10_1c_Verification_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757686bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_1['C10_1c_Verification_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bf9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering to only organizations with Complete verification status\n",
    "complete = df_10_1[df_10_1['C10_1c_Verification_status']=='Complete'].drop_duplicates(subset=['Account'])\n",
    "complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breakdown of Complete companies by verification type\n",
    "complete['C10_1c_Verification_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a9bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#streamline 10_1_c down to just the columns needed for merge\n",
    "mergers_10_1 = df_10_1[df_10_1['C10_1c_Verification_status']=='Complete']\n",
    "mergers_10_1.drop(['Public', 'Response_received_date','C10_1c_Statement','C10_1c_Reference', 'C10_1c_standard'], axis=1, inplace=True)\n",
    "mergers_10_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing above numbers to total number of organizations\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6e99e",
   "metadata": {},
   "source": [
    "These numbers indicate we would be able to use about 10% of the total accounts for predictions if we focused on reasonable assurance and up. It might be worthwhile to compare results of a model when the Limited Assurance category is included vs. excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examining proportion of emissions verified\n",
    "complete.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete['C10_1c_Proportion_of_emissions_verified']=pd.to_numeric(complete['C10_1c_Proportion_of_emissions_verified'])\n",
    "complete['C10_1c_Proportion_of_emissions_verified'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41720495",
   "metadata": {},
   "source": [
    "Out of the 1457 accounts with Complete verification status, 86% of them indicate 100% of their emissions have been verified. If that's the case, I wonder how meaningful is verification type/level of assurance? I'm thinking this might be a reasonable argument for using those 1249 accounts for further examination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac62f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot the 6_5 dataframe to get to focus on emissions by Account\n",
    "pivot_df = relevant_calc.pivot(index='Account', columns='RowName', values = ['C6_5_Metric_tonnes_CO2'])\n",
    "pivot_df.head(10)\n",
    "#and merge with the 10_1 verification on Account number; drop unneeded columns and start looking at calculation status, etc. by sector for the most \"reliable\" sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147255be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in NaN values with zeros\n",
    "pivot_df.fillna(0, inplace=True)\n",
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summing emissions values\n",
    "total_scope_3 = pivot_df\n",
    "\n",
    "total_scope_3['Total Scope 3 Emissions'] = total_scope_3.sum(axis=1)\n",
    "\n",
    "total_scope_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#streamline 10_1_c down to just the columns needed for merge\n",
    "mergers_10_1 = df_10_1[df_10_1['C10_1c_Verification_status']=='Complete'].copy()\n",
    "mergers_10_1.drop(['Public', 'Response_received_date','C10_1c_Statement','C10_1c_Reference', 'C10_1c_standard'], axis=1, inplace=True)\n",
    "mergers_10_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(mergers_10_1, total_scope_3 , on = 'Account', how = 'left')\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c44fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['C10_1c_Verification_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the handful of missing values in the Verification type category; the spreadsheet appears to have values for these, so not sure if it was an issue with the pivot. Still need to figure out how best to troubleshoot that part\n",
    "missing_verify = combined[combined['C10_1c_Verification_type'].isna()]\n",
    "pd.set_option('display.max_columns', 50)\n",
    "missing_verify.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors = combined\n",
    "df_sectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbdda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some unneeded columns.\n",
    "df_sectors.drop(['Primary_questionnaire_sector', 'C10_1c_Row', 'C10_1c_RowName', 'C10_1c_Scope_3_category', 'C10_1c_Verification_cycle', 'C10_1c_Verification_status'], axis=1, inplace=True)\n",
    "df_sectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09bb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming all columns to try to remove the parentheses issue post-merge\n",
    "df_sectors.columns = ['Account', 'Organization', 'Country', 'Primary_activity', 'Primary_sector', 'Primary_industry', 'C10_1c_Verification_type', 'C10_1c_Proportion_of_emissions_verified', \n",
    "                     'C6_5_Metric_tonnes_CO2, Business travel', 'C6_5_Metric_tonnes_CO2, Capital goods', 'C6_5_Metric_tonnes_CO2, Downstream leased assets', 'C6_5_Metric_tonnes_CO2, Downstream transportation and distribution',\n",
    "                     'C6_5_Metric_tonnes_CO2, Employee commuting', 'C6_5_Metric_tonnes_CO2, End of life treatment of sold products', 'C6_5_Metric_tonnes_CO2, Franchises', 'C6_5_Metric_tonnes_CO2, Fuel-and-energy-related activities_not included in Scope 1 or 2',\n",
    "                     'C6_5_Metric_tonnes_CO2, Investments', 'C6_5_Metric_tonnes_CO2, Other_downstream', 'C6_5_Metric_tonnes_CO2, Other_upstream', 'C6_5_Metric_tonnes_CO2, Processing of sold products', 'C6_5_Metric_tonnes_CO2, Purchased goods and services', 'C6_5_Metric_tonnes_CO2, Upstream leased assets',\n",
    "                     'C6_5_Metric_tonnes_CO2, Upstream transportation and distribution', 'C6_5_Metric_tonnes_CO2, Use of sold products', 'C6_5_Metric_tonnes_CO2, Waste generated in operations', 'Total_Scope3_Emissions']\n",
    "df_sectors['Primary_sector'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing for differences by verification status, starting with financial services since it has the most orgs\n",
    "financial_svcs = df_sectors[df_sectors['Primary_sector'] == 'Financial services']\n",
    "financial_svcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_svcs['C10_1c_Verification_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fs = financial_svcs[financial_svcs['C10_1c_Verification_type'].isnull()]\n",
    "missing_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f455002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling in the few missing values with \"Unknown\" for verification status\n",
    "financial_svcs['C10_1c_Verification_type'].fillna('Unknown', inplace=True)\n",
    "financial_svcs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking emissions data distribution for normality to determine parametric (ANOVA) vs non-parametric test to compare differences\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='C10_1c_Verification_type', y='Total_Scope3_Emissions', data=financial_svcs, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683eab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "res = stats.probplot(financial_svcs['Total_Scope3_Emissions'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running Kruskal-Wallis test to check for significant differences by verification status\n",
    "unknown = financial_svcs['C10_1c_Verification_type'] == 'Unknown'\n",
    "limited = financial_svcs['C10_1c_Verification_type'] == 'Limited assurance'\n",
    "reasonable = financial_svcs['C10_1c_Verification_type'] == 'Reasonable assurance'\n",
    "moderate = financial_svcs['C10_1c_Verification_type'] == 'Moderate assurance'\n",
    "high = financial_svcs['C10_1c_Verification_type'] == 'High assurance'\n",
    "underway = financial_svcs['C10_1c_Verification_type'] == 'Third party verification/ assurance underway'\n",
    "stats.kruskal(unknown, limited, reasonable, moderate, high, underway)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3a5f2",
   "metadata": {},
   "source": [
    "Kruskal-Wallis test was significant, indicating emissions data within Financial Services differs significantly by verification status. Based on these results we'll want to focus on either establishing some scoring criteria or limiting the data we use for prediction to the top 1-2 levels of assurance (or a bit of both).\n",
    "\n",
    "Based on further discussion with the project team and company contact, I shifted focus to work on identifying the sectors with high “Purchased goods and services” (Category 1) and “Use of sold products” (Category 11) emissions. I set out to identify three main items:\n",
    "\n",
    "1. Sectors with the highest Category 1 and 11 emissions\n",
    "2. Sectors with the most companies reporting Category 1 and 11 emissions\n",
    "3. Sectors with the highest percentage of companies reporting Category 1 and 11 emissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d80b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering to focus on Purchased goods and services and Use of sold products columns\n",
    "focus_cats = relevant_calc.loc[relevant_calc['RowName'].isin(['Purchased goods and services', 'Use of sold products'])]\n",
    "focus_cats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some excess columns\n",
    "focus_cats = focus_cats[['Account', 'Organization', 'Country', 'Primary_sector', 'RowName', 'C6_5_Evaluation_status', 'C6_5_Metric_tonnes_CO2', 'C6.5_Percentage_missions_calculated' ]]\n",
    "focus_cats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19772e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking/addressing missing values\n",
    "focus_cats.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_cats.fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35825a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pivot table grouping CO2 values by sector and category\n",
    "focus_pivot = pd.pivot_table(focus_cats, values='C6_5_Metric_tonnes_CO2', index=['Primary_sector'], columns = ['RowName'], aggfunc=np.sum)\n",
    "focus_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f52dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a column to sum the values\n",
    "focus_pivot['Total CO2e'] = focus_pivot.sum(axis=1)\n",
    "focus_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppressing scientific notation\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "focus_pivot.sort_values(by='Total CO2e', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d787a",
   "metadata": {},
   "source": [
    "Sectors with the highest CO2 emissions reported in our focus categories:\n",
    "\n",
    "1) Electrical & electronic equipment \n",
    "2) Oil and gas processing \n",
    "3) Transportation equipment \n",
    "4) Powered machinery \n",
    "5) Chemicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sectors with the most companies reporting Category 1 and 11 emissions\n",
    "focus_orgs = relevant_calc.loc[relevant_calc['RowName'].isin(['Purchased goods and services', 'Use of sold products'])]\n",
    "focus_orgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7716dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_orgs.groupby(by='Primary_sector').nunique().sort_values(by='Organization', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sectors with the largest number of organizations reporting in key categories are:\n",
    "1) Electrical & electronic equipment\n",
    "2) Financial services\n",
    "3) Chemicals\n",
    "4) Food & beverage processing\n",
    "5) Powered Machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adeef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting total list of orgs\n",
    "total_orgs = relevant_calc.drop_duplicates(subset='Organization')\n",
    "total_orgs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241dc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the rows with the focus categories into the total_orgs dataframe\n",
    "total_orgs = pd.concat([total_orgs, focus_orgs], ignore_index=True)\n",
    "total_orgs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f99f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding column to flag if org reported emissions in the focus categories\n",
    "total_orgs['FocusCategory'] = np.where(total_orgs['RowName'].isin(['Purchased goods and services', 'Use of sold products']), True, False)\n",
    "total_orgs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d884de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicates so we can calculate percentage accurately\n",
    "total_orgs.drop_duplicates('Organization', keep='last')\n",
    "total_orgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orgs.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb94854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating percentage reporting focus category emissions by sector\n",
    "percentages = total_orgs.pivot_table(index='Primary_sector', values = ['FocusCategory', 'RowName'], aggfunc={'FocusCategory': 'sum', 'RowName': 'count'})\n",
    "percentages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages['PercentofTotal'] = percentages['FocusCategory']/percentages['RowName']\n",
    "percentages.sort_values('PercentofTotal', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab618462",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages['PercentofTotal'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages['PercentofTotal'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f573a9",
   "metadata": {},
   "source": [
    "Top 5 are very different when we look at percent of total organizations reporting emissions in the focus categories:\n",
    "\n",
    "1) Logging & rubber tapping (2 of 2 orgs) \n",
    "2) Crop farming (8 of 8 orgs) \n",
    "3) Rail transport (17 of 18 orgs) \n",
    "4) Tobacco (13 of 14 orgs) \n",
    "5) Fish & animal farming (10 of 11 orgs)\n",
    "\n",
    "But the total number of orgs reporting emissions in those first 5 categories are all relatively small. If we look at the sectors with the largest number of orgs reporting in focus categories, percent of total orgs report are as follows:\n",
    "\n",
    "1) Electrical & electronic equipment (66%) \n",
    "2) Financial services (66%) \n",
    "3) Chemicals (82%) \n",
    "4) Food & beverage processing (77%) \n",
    "5) Powered Machinery (66%)\n",
    "\n",
    "All are at or above the median value for % reporting. We could potentially focus on a few of the largest sectors, and/or consider any sectors above that median % reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing to csv files\n",
    "percentages.to_csv('percentages.csv')
total_orgs.to_csv('total_orgs.csv')
focus_orgs.to_csv('focus_orgs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef8145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate column identifying organizations that reported emissions in both focus categories\n",
    "x=0\n",
    "result = []\n",
    "for value in relevant_calc['RowName']:\n",
    "    if value in(['Purchased goods and services']):\n",
    "      result.append(x+1)\n",
    "    elif value in (['Use of sold products']):\n",
    "      result.append(x+1)\n",
    "    else:\n",
    "        result.append(x)\n",
    "      \n",
    "relevant_calc['FocusCats'] = result\n",
    "relevant_calc.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0092710",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_cats = relevant_calc.loc[relevant_calc['FocusCats'] >= 1]\n",
    "focus_cats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052144b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_cats = focus_cats[focus_cats.duplicated('Organization')]\n",
    "both_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ecb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_cats['FocusCats'] = 2\n",
    "both_cats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81097260",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_final = pd.concat([focus_cats, both_cats])\n",
    "focus_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_final.drop_duplicates('Organization', keep='last', inplace=True)\n",
    "focus_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_final.drop(['Row', 'RowName', 'C6_5_Metric_tonnes_CO2', 'C6.5_Percentage_missions_calculated', 'C6.5_Exclusions'], axis=1, inplace=True)\n",
    "focus_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing list to csv files\n",
    "percentages.to_csv('percentages.csv')
total_orgs.to_csv('total_orgs.csv')
focus_orgs.to_csv('focus_orgs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa3827",
   "metadata": {},
   "source": [
    "At this point in the project, we received an additional data source reporting annual revenue to incorporate into our analysis and modeling efforts. One of my teammates focused on merging in the financial data. Because the revenue data came from a completely separate source, it took considerable work to match up revenue data with the companies in our dataset. Because we opted to only use companies with revenue info in our modeling attempts, our dataset got considerably smaller (810 companies compared to nearly 3,000 in my previous \"focus orgs\" dataset). Another teammate had developed a \"trust score\" methodology based on the different verification criteria available to us in the dataset, so we incorporated it into our dataset as a potential model feature.\n",
    "\n",
    "I started with some visualization and examining outliers in our updated dataset to help determine a modeling plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_810_frame_with_trust_score.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a5cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['primary_sector'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e84fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_sectors = df[df['primary_sector'].isin(['Electrical & electronic equipment', 'Financial services', 'Chemicals','Food & beverage processing','Powered machinery'])]\n",
    "sns.set_style(\"ticks\")\n",
    "f = plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=focus_sectors, x='primary_sector', y='cat_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929283d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR cat_1\n",
    "Q1 = np.percentile(focus_sectors['cat_1'], 25,\n",
    "                   interpolation = 'midpoint')\n",
    " \n",
    "Q3 = np.percentile(focus_sectors['cat_1'], 75,\n",
    "                   interpolation = 'midpoint')\n",
    "IQR = Q3 - Q1\n",
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = focus_sectors[focus_sectors['cat_1']> Q3]\n",
    "outliers['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers['primary_activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers['primary_industry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers['verification_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_check = outliers[outliers['Percent S3 verified FILTERED']>0]\n",
    "trust_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0267fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=focus_sectors, x='primary_sector', y='cat_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dacdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR cat_11\n",
    "Q1_11 = np.percentile(focus_sectors['cat_11'], 25,\n",
    "                   interpolation = 'midpoint')\n",
    " \n",
    "Q3_11 = np.percentile(focus_sectors['cat_11'], 75,\n",
    "                   interpolation = 'midpoint')\n",
    "IQR = Q3_11 - Q1_11\n",
    "Q3_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_11 = focus_sectors[focus_sectors['cat_11']> Q3]\n",
    "outliers_11['verification_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns I definitely won't want in the model. Keeping one Organization column for now in case it helps to examine outliers, etc. if needed\n",
    "df_trimmed = df.copy()\n",
    "df_trimmed.drop(['Unnamed: 0', 'account_number', 'organization', 'statement', 'Account'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4bfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting selected object columns to category codes\n",
    "df_quant = df_trimmed.copy()\n",
    "df_quant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['country', 'primary_activity', 'primary_sector', 'primary_industry', 'verification_type']:\n",
    "  df_quant[col] = df_quant[col].astype('category')\n",
    "df_quant.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['country', 'primary_activity', 'primary_sector', 'primary_industry', 'verification_type']:\n",
    "  df_quant[col] = df_quant[col].cat.codes\n",
    "df_quant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping remaining object and total emissions columns\n",
    "df_quant.drop(['verification_cycle_in_place', 'report_status', 'Organization', 'total_emissions'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modeling libraries and scaling the features\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "fs_reg = scaler.fit_transform(df_quant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f28d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = pd.DataFrame(fs_reg, columns=['country', 'primary_activity', 'primary_sector', 'primary_industry',\n",
    "       'cat_6', 'cat_2', 'cat_13', 'cat_9', 'cat_7', 'cat_12', 'cat_14',\n",
    "       'cat_3', 'cat_15', 'cat_17', 'cat_16', 'cat_10', 'cat_1', 'cat_8',\n",
    "       'cat_4', 'cat_11', 'cat_5', 'revenue', 'verification_type',\n",
    "       'revenue / emissions', 'Percent S3 verified',\n",
    "       'Percent S3 verified FILTERED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d12115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at correlations with category 1 and category 11\n",
    "correlation_matrix_1 = scaled.corr()\n",
    "correlation_matrix_1['cat_1'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f142050",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_11 = scaled.corr()\n",
    "correlation_matrix_11['cat_11'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3dc840",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "_ = sns.heatmap(scaled.corr(), ax=ax, annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef482c",
   "metadata": {},
   "source": [
    "Because we are trying to predict a continuous variable (CO2 emissions), I decided to start with multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running regression model starting with predicting category 1 emissions. I'm not using other categories as features initially, but there are a couple that are highly correlated (cat_4, cat_5, and cat_7)\n",
    "y = scaled['cat_1']\n",
    "x = scaled[['primary_activity', 'primary_sector', 'primary_industry', 'country', 'revenue', 'Percent S3 verified FILTERED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc829b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training/testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
    "#MAE\n",
    "print(\"Mean absolute error: %.2f\" % mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de494f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting residuals\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "visualizer = ResidualsPlot(regr)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal residual QQ plot\n",
    "import statsmodels.api as sm\n",
    "fig = sm.qqplot(x, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b670a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots indicate some heteroscedasticity; trying log transformation on the dependent variable (although wondering if this is not appropriate since the data is already scaled)\n",
    "ylog = np.log1p(y)\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "#create histograms\n",
    "axs[0].hist(y.dropna(), edgecolor='blue')\n",
    "axs[1].hist(ylog, edgecolor='blue')\n",
    "\n",
    "#add title to each histogram\n",
    "axs[0].set_title('Original Data')\n",
    "axs[1].set_title('Log-Transformed Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-running model with log tranformation \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, ylog, test_size=0.33, random_state=42)\n",
    "\n",
    "# Create linear regression object\n",
    "regr_log = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr_log.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = regr_log.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", regr_log.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred2))\n",
    "#MAE\n",
    "print(\"Mean absolute error: %.2f\" % mean_absolute_error(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b113b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(regr_log)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying Lasso Regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "# define model\n",
    "model = Lasso(alpha=1.0)\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0fd3f",
   "metadata": {},
   "source": [
    "The Lasso regression performed well (Mean MAE: 0.388), but was using raw Scope 3 emissions values rather than emissions adjusted for revenue (metric tonnes per $1 million in annual revenue). We had agreed to use the revenue-adjusted metric as our outcome value going forward, so my next step was re-running the models with adjusted category 1/category 11 values as the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_810_frame_with_trust_score_emissions_adjusted.csv')\n",
    "#filter down to columns to be used for modeling\n",
    "model_df = df[['primary_sector', 'Percent S3 verified', 'Percent S3 verified FILTERED', 'cat_1_adj', 'cat_2_adj', 'cat_3_adj', 'cat_4_adj',\n",
    "               'cat_5_adj', 'cat_6_adj', 'cat_7_adj', 'cat_8_adj', 'cat_9_adj', 'cat_10_adj', 'cat_11_adj', 'cat_12_adj', 'cat_13_adj', 'cat_14_adj',\n",
    "               'cat_15_adj', 'cat_16_adj', 'cat_17_adj']]\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_sectors = model_df[model_df['primary_sector'].isin(['Electrical & electronic equipment', 'Financial services', 'Chemicals','Food & beverage processing','Powered machinery'])]\n",
    "sns.set_style(\"ticks\")\n",
    "f = plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=focus_sectors, x='primary_sector', y='cat_1_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR cat_1 - getting these values for possible later use\n",
    "Q1_1 = np.percentile(focus_sectors['cat_1_adj'], 25,\n",
    "                   interpolation = 'midpoint')\n",
    " \n",
    "Q3_1 = np.percentile(focus_sectors['cat_1_adj'], 75,\n",
    "                   interpolation = 'midpoint')\n",
    "IQR = Q3_1 - Q1_1\n",
    "Q3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2082d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_sectors.reset_index()\n",
    "focus_sectors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing what our biggest oulier is for category 1 and category 11\n",
    "extreme = focus_sectors[focus_sectors['cat_1_adj'] > 30000]\n",
    "extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_11 = focus_sectors[focus_sectors['cat_11_adj'] > 30000]\n",
    "extreme_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying a model with our two most extreme outliers dropped\n",
    "focus_sectors_trimmed = focus_sectors[focus_sectors['cat_11_adj']< 30000]\n",
    "focus_sectors_trimmed.reset_index(inplace=True)\n",
    "focus_sectors_trimmed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0de8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=focus_sectors, x='primary_sector', y='cat_11_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fc314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR cat_11\n",
    "Q1_11 = np.percentile(focus_sectors['cat_11_adj'], 25,\n",
    "                   interpolation = 'midpoint')\n",
    " \n",
    "Q3_11 = np.percentile(focus_sectors['cat_11_adj'], 75,\n",
    "                   interpolation = 'midpoint')\n",
    "IQR = Q3_11 - Q1_11\n",
    "Q3_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e992e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_sectors_trimmed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f948499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate into x and y dataframes for features and target\n",
    "y1 = focus_sectors_trimmed['cat_1_adj']\n",
    "x = focus_sectors_trimmed[['Percent S3 verified', 'Percent S3 verified FILTERED', 'cat_2_adj', 'cat_3_adj', 'cat_4_adj', 'cat_5_adj',\n",
    "       'cat_6_adj', 'cat_7_adj', 'cat_8_adj', 'cat_9_adj', 'cat_10_adj',\n",
    "       'cat_11_adj', 'cat_12_adj', 'cat_13_adj', 'cat_14_adj', 'cat_15_adj',\n",
    "       'cat_16_adj', 'cat_17_adj']]\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "fs_reg = scaler.fit_transform(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = pd.DataFrame(fs_reg, columns=['Percent S3 verified',\n",
    "       'Percent S3 verified FILTERED', 'cat_2_adj', 'cat_3_adj', 'cat_4_adj', 'cat_5_adj', 'cat_6_adj', 'cat_7_adj', 'cat_8_adj', 'cat_9_adj',\n",
    "       'cat_10_adj', 'cat_11_adj', 'cat_12_adj', 'cat_13_adj', 'cat_14_adj',  'cat_15_adj',  'cat_16_adj', 'cat_17_adj'])\n",
    "scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding back primary sector\n",
    "sectors = focus_sectors_trimmed['primary_sector']\n",
    "sectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36332360",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = scaled.join(sectors)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables\n",
    "df_dummies = combined_df\n",
    "df_dummies = pd.get_dummies(df_dummies, columns=['primary_sector'], prefix=['sector'])\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed822e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training/testing sets\n",
    "x2 = df_dummies\n",
    "X_train, X_test, y_train, y_test = train_test_split(x2, y1, test_size=0.33, random_state=42)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
    "#MAE\n",
    "print(\"Mean absolute error: %.2f\" % mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying log scale transformation since model metrics (RMSE, MAE) are poor\n",
    "xlog = np.log1p(x2)\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "#create histograms\n",
    "axs[0].hist(x2.dropna(), edgecolor='blue')\n",
    "axs[1].hist(xlog, edgecolor='blue')\n",
    "\n",
    "#add title to each histogram\n",
    "axs[0].set_title('Original Data')\n",
    "axs[1].set_title('Log-Transformed Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2facfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-running model with log tranformation \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xlog, y1, test_size=0.33, random_state=42)\n",
    "\n",
    "# Create linear regression object\n",
    "regr_log = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr_log.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred2 = regr_log.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", regr_log.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred2))\n",
    "#MAE\n",
    "print(\"Mean absolute error: %.2f\" % mean_absolute_error(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a098770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying SVM because MSE/MAE are still really high\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "X_SVMtrain, X_SVMtest, y_SVMtrain, y_SVMtest = train_test_split(x2, y1, test_size=0.33, random_state=42)\n",
    "clf = svm.SVR(kernel=\"rbf\", C=50, gamma=\"auto\", degree=3, epsilon=0, coef0=0.1)\n",
    "clf.fit(X_SVMtrain, y_SVMtrain)\n",
    "print('R2 on test data:', + clf.score(X_SVMtest, y_SVMtest))\n",
    "y_pred = clf.predict(X_SVMtest)\n",
    "print('RMSE of prediction:', + mse(y_pred, y_SVMtest, squared = False))\n",
    "print('mean of y_test:', + y_SVMtest.mean())\n",
    "print('MAE', + mean_absolute_error(y_pred, y_SVMtest))\n",
    "res = y_pred - y_SVMtest\n",
    "plt.scatter(y_pred, res)\n",
    "plt.show()\n",
    "plt.scatter(y_pred, y_SVMtest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from numpy import absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=1.0)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, x2, y1, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#still really high MAE values. Revisiting the correlation matrix to potentially trim some features\n",
    "correlation_matrix = focus_sectors_trimmed.corr()\n",
    "correlation_matrix['cat_1_adj'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trim = x2[['cat_2_adj', 'cat_15_adj', 'cat_14_adj', 'cat_4_adj', 'cat_5_adj']]\n",
    "x_trim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c22d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-running lasso with trimmed feature set\n",
    "model = Lasso(alpha=1.0)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, x_trim, y1, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2004618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAE is still high; trying Ridge regression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgemodel = Ridge(alpha=1.0)\n",
    "# evaluate model\n",
    "scores = cross_val_score(ridgemodel, x_trim, y1, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrying SVM with trimmed feature set\n",
    "X_SVMtrain, X_SVMtest, y_SVMtrain, y_SVMtest = train_test_split(x_trim, y1, test_size=0.33, random_state=42)\n",
    "clf = svm.SVR(kernel=\"rbf\", C=50, gamma=\"auto\", degree=3, epsilon=0, coef0=0.1)\n",
    "clf.fit(X_SVMtrain, y_SVMtrain)\n",
    "print('R2 on test data:', + clf.score(X_SVMtest, y_SVMtest))\n",
    "y_pred = clf.predict(X_SVMtest)\n",
    "print('RMSE of prediction:', + mse(y_pred, y_SVMtest, squared = False))\n",
    "print('mean of y_test:', + y_SVMtest.mean())\n",
    "print('MAE', + mean_absolute_error(y_pred, y_SVMtest))\n",
    "res = y_pred - y_SVMtest\n",
    "plt.scatter(y_pred, res)\n",
    "plt.show()\n",
    "plt.scatter(y_pred, y_SVMtest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4652a00",
   "metadata": {},
   "source": [
    "Mean Absolute Error (MAE) scores continued to be pretty high on all of the regression and SVM modeling attempts, so none of these model options were looking promising. The spread of the dataset seemed to be a possible issue, given that there were a handful of organizations that had extremely high adjusted category 1 and 11 emissions values. There are enough that it didn't make sense to consider them truly outliers and adjust or throw them out, particularly considering we know there were hundreds more companies that had reported emissions data but couldn't be matched to revenue data; it's possible those gaps could get filled in with more data. I decided to explore binning our target values as a potential solution. I thought if we could bin the adjusted emissions values into reasonably meaningful ranges, it would still be helpful to our company contact and would enable me to try out some classifier models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_810_frame_with_trust_score_emissions_adjusted.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Data_Year'] = 2020\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7494e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat_1_adj'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding in 2021 data\n",
    "df_21 = pd.read_csv('2021_data.csv')\n",
    "df_21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c180e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all columns aligned and names consistent for merging\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09095112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'revenue': 'revenue_final'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5734d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20 = df\n",
    "df_20.drop(['cat_6',\n",
    "       'cat_2', 'cat_13', 'cat_9', 'cat_7', 'cat_12', 'cat_14', 'cat_3',\n",
    "       'cat_15', 'cat_17', 'cat_16', 'cat_10', 'cat_1', 'cat_8', 'cat_4',\n",
    "       'cat_11', 'cat_5', 'verification_cycle_in_place',\n",
    "       'report_status', 'verification_type', 'statement', 'total_emissions',\n",
    "       'revenue / emissions', 'Account', 'Organization', 'Percent S3 verified',\n",
    "       'Percent S3 verified FILTERED'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0451861",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20.rename(columns={'revenue': 'revenue_final'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a407444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_21.rename(columns={'country_area': 'country'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_21.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_years = pd.concat([df, df_21], ignore_index=True)\n",
    "df_both_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e673de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_years['cat_1_adj'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_21['cat_1_adj'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limiting to the five focus sectors we had identified as having a large number/high percentage of companies reporting category 1 and 11 emissions and dummy coding categorical variables\n",
    "x_bin = df_both_years[df_both_years['primary_sector'].isin(['Electrical & electronic equipment', 'Financial services', 'Chemicals','Food & beverage processing','Powered machinery'])].copy()\n",
    "x_bin = pd.get_dummies(x_bin, columns=['country', 'primary_sector'], prefix=['country_', 'sect_'])\n",
    "x_bin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at distribution of cat_1_adj to determine bin values\n",
    "x_bin['cat_1_adj'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin['bins'] = pd.cut(x=x_bin['cat_1_adj'], bins=[0, .001, 50, 250, 1000000],\n",
    "                    labels=['No Category 1', '< 50 MT per million', '<250 MT per million', '250 MT per million or more'])\n",
    "x_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe90f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin.bins.fillna('No Category 1', inplace=True)\n",
    "x_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin['bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61657211",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb71f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into features and targets before scaling\n",
    "y_class = x_bin['bins']\n",
    "x_class = x_bin\n",
    "x_class.drop(['Unnamed: 0', 'account_number', 'organization', 'primary_activity',\n",
    "       'primary_industry', 'revenue_final', 'cat_1_adj', 'bins'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbaa3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = x_class.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ba5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "bin_scale = scaler.fit_transform(x_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_class_scaled = pd.DataFrame(bin_scale, columns=cols)\n",
    "x_class_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying an ADABoost model first because a teammate had seen a bit of promise in another modeling attempt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a02383",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_class_scaled, y_class, test_size=0.2)\n",
    "abc = AdaBoostClassifier(n_estimators=50,\n",
    "                         learning_rate=1)\n",
    "abc_model = abc.fit(X_train, y_train) \n",
    "y_pred = abc_model.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying Random Forest classifier since ADABoost accuracy was low\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=4, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "print(rfc.score(X_train, y_train))\n",
    "print(rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9da9c",
   "metadata": {},
   "source": [
    "The Random Forest showed some promising accuracy scores (initially .80 for training and .75 for test). Values shifted further apart when train/test split was rerun; not surprising given this dataset is on the small side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22054e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at feature importances\n",
    "import sys\n",
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5fc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.estimators import plot_feature_importances\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 7)\n",
    "plot_feature_importances(rfc, feature_names=x_class_scaled.columns, x_tick_rotation=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0abd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some features just to see if it helps the training scores improve at all\n",
    "x_reduced = x_class[['cat_4_adj', 'cat_3_adj', 'sect__Financial services', 'cat_5_adj','cat_6_adj', 'cat_2_adj', 'cat_12_adj', 'cat_11_adj', 'cat_9_adj', 'cat_7_adj', 'cat_6_adj']]\n",
    "x_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(x_reduced, y_class, test_size=0.2)\n",
    "rfc2 = RandomForestClassifier(max_depth=4, random_state=42)\n",
    "rfc2.fit(X_train2, y_train2)\n",
    "\n",
    "print(rfc2.score(X_train2, y_train2))\n",
    "print(rfc2.score(X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ca004",
   "metadata": {},
   "source": [
    "The train/test scores are a bit closer than the original model run (.79 train, .73 test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying the original Random Forest model for predicting Category 11 values\n",
    "x_bin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin_11 = df_both_years[df_both_years['primary_sector'].isin(['Electrical & electronic equipment', 'Financial services', 'Chemicals','Food & beverage processing','Powered machinery'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592399e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin_11.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin_11 = pd.get_dummies(x_bin_11, columns=['country', 'primary_sector'], prefix=['country_', 'sect_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b737d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin_11['bins'] = pd.cut(x=x_bin_11['cat_11_adj'], bins=[0, .001, 50, 250, 1000000],\n",
    "                    labels=['No Category 11', '< 50 MT per million', '<250 MT per million', '250 MT per million or more'])\n",
    "x_bin_11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904078b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin_11.bins.fillna('No Category 11', inplace=True)\n",
    "x_bin_11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c54324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin_11['bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin_11.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class_11 = x_bin_11['bins']\n",
    "x_class_11 = x_bin_11\n",
    "x_class_11.drop(['Unnamed: 0', 'account_number', 'organization', 'primary_activity',\n",
    "       'primary_industry', 'revenue_final', 'cat_11_adj','bins'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823668d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_11 = x_class_11.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_11 = StandardScaler()\n",
    "bin_scale_11 = scaler.fit_transform(x_class_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_class_scaled_11 = pd.DataFrame(bin_scale_11, columns=cols_11)\n",
    "x_class_scaled_11.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class_11.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63abbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_11, X_test_11, y_train_11, y_test_11 = train_test_split(x_class_scaled_11, y_class_11, test_size=0.2)\n",
    "\n",
    "rfc_11 = RandomForestClassifier(max_depth=4, random_state=42)\n",
    "rfc_11.fit(X_train_11, y_train_11)\n",
    "\n",
    "print(rfc_11.score(X_train_11, y_train_11))\n",
    "print(rfc_11.score(X_test_11, y_test_11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64416fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20, 7)\n",
    "plot_feature_importances(rfc_11, feature_names=x_class_scaled_11.columns, x_tick_rotation=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb0199",
   "metadata": {},
   "source": [
    "RFC performed reasonably on Category 11 as well, although the bins are less balanced (a lot more zeroes in Category 11). Accuracy scores again shifted a bit when I reran the code with a fresh train/test set.\n",
    "\n",
    "We discussed potential issues with including both the 2020 and 2021 data in the model because some companies have data for both years (essentially, should be paired samples) while other do not. So I decided to try re-running the models on the 2020 and 2021 data separately to see how they performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rerunning model on 2020 and 2021 data separately\n",
    "x_class_scaled_20 = x_class_scaled[x_class_scaled['Data_Year'] < 1]\n",
    "x_class_scaled_20.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bin['Data_Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3914300",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class_20 = y_class[0:328]\n",
    "y_class_20.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(x_class_scaled_20, y_class_20, test_size=0.2)\n",
    "\n",
    "rfc_20 = RandomForestClassifier(max_depth=4, random_state=42)\n",
    "rfc_20.fit(X_train_20, y_train_20)\n",
    "\n",
    "print(rfc_20.score(X_train_20, y_train_20))\n",
    "print(rfc_20.score(X_test_20, y_test_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class_21 = y_class[328::]\n",
    "y_class_21.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57396e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_class_scaled_21 = x_class_scaled[x_class_scaled['Data_Year'] > 1]\n",
    "x_class_scaled_21.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_21, X_test_21, y_train_21, y_test_21 = train_test_split(x_class_scaled_21, y_class_21, test_size=0.2)\n",
    "\n",
    "rfc_21 = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "rfc_21.fit(X_train_21, y_train_21)\n",
    "\n",
    "print(rfc_21.score(X_train_21, y_train_21))\n",
    "print(rfc_21.score(X_test_21, y_test_21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b5ea2",
   "metadata": {},
   "source": [
    "The model worked reasonably well on 2021 data (.76 on training, .75 on test), but had overfitting issues on the 2020 data alone. Additional data from previous years, if available, could help indicate whether 2020 is an anomaly year (very possible with COVID impacts and supply chain issues).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
